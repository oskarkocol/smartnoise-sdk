name: Redshift Integration Tests
on:
  pull_request:
    paths:
      - 'sql/tests/**'
      - 'sql/snsql/**'
      - 'sql/pyproject.toml'
  workflow_dispatch:

jobs:
  container-job:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: [3.9]
      max-parallel: 5

    steps:
      - uses: actions/checkout@v2
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1
        with:
          terraform_version: 0.14.0

      - name: Set up miniconda
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniconda-version: "latest"
          auto-update-conda: true
          auto-activate-base: true
          python-version: ${{ matrix.python-version }}

      - name: Upgrade pip
        shell: bash -l {0}
        run: |
          conda install pip
          conda update pip

      - name: Setup SDK
        shell: bash -l {0}
        run: |
          cd sql
          pip install --no-cache-dir -r tests/requirements.txt
          pip install --no-cache-dir -r tests/setup/bigquery/requirements.txt
          pip install --no-cache-dir  . # sdk

      - name: Download test datasets
        shell: bash -l {0}
        run: |
          cd sql
          python tests/check_databases.py
          cd ..
          cp datasets/PUMS_large.csv datasets/PUMS_large.csv.bak
          sed 's/"//g' datasets/PUMS_large.csv.bak > datasets/PUMS_large.csv

      - name: Terraform Init - Redshift dataset
        env:
          AWS_ACCESS_KEY_ID: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          AWS_SECRET_ACCESS_KEY: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
        run: |
          cd sql/tests/terraform/redshift
          terraform init \
          -var 'region=${{ secrets.AWS_REGION }}' \
          -var 'bucket_name=${{ secrets.AWS_BUCKET_NAME }}'

      - name: Terraform Plan - Redshift dataset
        env:
          AWS_ACCESS_KEY_ID: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          AWS_SECRET_ACCESS_KEY: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
        run: |
          cd sql/tests/terraform/redshift
          terraform plan \
          -var 'region=${{ secrets.AWS_REGION }}' \
          -var 'aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}' \
          -var 'aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}'

      - name: Terraform Apply - Redshift dataset
        env:
          AWS_ACCESS_KEY_ID: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          AWS_SECRET_ACCESS_KEY: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
        continue-on-error: true
        run: |
          cd sql/tests/terraform/redshift
          terraform apply \
          -auto-approve \
          -var 'region=${{ secrets.AWS_REGION }}' \
          -var 'aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}' \
          -var 'aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}'

      # - name: Test SDK
      #   continue-on-error: true
      #   env:
      #     GOOGLE_PROJECT_ID: "${{ secrets.GOOGLE_PROJECT_ID }}"
      #     GOOGLE_APPLICATION_CREDENTIALS: "${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}"
      #   shell: bash -l {0}
      #   run: |
      #     cd sql
      #     mkdir ~/.smartnoise
      #     cp tests/setup/bigquery/connections-unit.yaml ~/.smartnoise/connections-unit-template.yaml
      #     sed "s/{secrets.GOOGLE_PROJECT_ID}/$GOOGLE_PROJECT_ID/g" ~/.smartnoise/connections-unit-template.yaml > ~/.smartnoise/connections-unit.yaml
      #     export SKIP_PANDAS=1
      #     pytest tests

      - name: Terraform Destroy - Redshift dataset
        env:
          AWS_ACCESS_KEY_ID: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          AWS_SECRET_ACCESS_KEY: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
        continue-on-error: true
        run: |
          cd sql/tests/terraform/redshift
          terraform destroy \
          -auto-approve \
          -var 'region=${{ secrets.AWS_REGION }}' \
          -var 'aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}' \
          -var 'aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}'
